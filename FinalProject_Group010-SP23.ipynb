{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Risk Assessment Using Machine Learning Techniques and an Ensemble Approach\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Jason Liang\n",
    "- Tasnia Jamal\n",
    "- Thanh Derek Nguyen\n",
    "- Wilson Chen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents \n",
    "- the solution/what you did\n",
    "- major results you came up with (mention how results are measured) \n",
    "\n",
    "__NB:__ this final project form is much more report-like than the proposal and the checkpoint. Think in terms of writing a paper with bits of code in the middle to make the plots/tables\n",
    "\n",
    "Creditability helps assist financial institutions in making decisions regarding the granting of credit. The credit standing for individuals is typically based on a multitude of factors and requires nuanced computational methods to ensure accurate predictions, especially in a turbulent market when bestowers need to seriously consider whether a person is worthy of loans. The goal of this research project is to predict creditability standing for individuals based on a variety of variables like duration of credit, account balance, occupation, and other personal financial statistics. Our workflow consists of cleaning the data and normalizing it, and then used to build supervised machine learning models like linear regression, logistic regression, decision trees, and K-nearest neighbors and then using obtained performance and error metrics, to conclude which model is best for classifying a person's credit score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "In finances, credit scoring is a way of analyzing statistical data used in financial organizations and banks to acquire a person's creditworthiness. The score itself plays a significant role in determining the creditworthiness of a person and if they are qualified to sanction a loan from the bank. The process of credit scoring is normally automated using machine learning techniques and involves evaluating various factors related to a person's or entity's credit history and financial behavior to determine the likelihood of repaying debts and fulfilling financial obligations <a name=\"abdou\"></a>[<sup>[1]</sup>](#abdounote).\n",
    "\n",
    "The primary purpose of credit scoring is to help lenders make informed decisions about extending credit to potential borrowers, allowing them to evaluate the risk associated with lending money to an individual or business. This allows them to determine the terms and conditions of the pending credit offer, including interest rates and credit limits. In common credit scoring models, the following are some of the main criteria used when calculating a score:\n",
    "\n",
    "1.) Payment history: This factor assesses the borrower's track record of making payments on time, including any late or missed payments.\n",
    "\n",
    "2.) Credit utilization: This measures the amount of credit a borrower is using in relation to their available credit limits. Higher credit limit may negatively impact overall credit score.\n",
    "\n",
    "3.) Length of credit history: The length of time the borrower has been using credit is taken into account. A longer credit history is typically viewed positively.\n",
    "\n",
    "4.) Credit mix: This factor considers the borrower's mix of credit accounts, such as credit cards mortgages, and loans. Having a diverse credit mix may be seen as positive.\n",
    "\n",
    "5.) New credit applications: Recent applications for credit or loans may have an impact on the credit score, as multiple applications within a short period can be viewed as a sign of financial instability <a name=\"xiao\"></a>[<sup>[2]</sup>](#xiaonote).\n",
    "\n",
    "The model then assigns a numerical value to each factor and generates a score (typically via machine learning models) that ranges from a minimum to a maximum value. The most commonly used scoring system is the FICO score, developed by the Fair Isaac Corporation <a name=\"fico\"></a>[<sup>[3]</sup>](#ficonote). It ranges from 300 to 850, with higher scores indicating lower credit risk. With the recent failure of Silicon Valley Bank and the decline of major tech giants in what appears to be a recession, it is more important than ever to develop accurate computational models that assist financial institutions in the decision-making process at the time of a customer's financial request. Today, the credit market demands new tools and technologies that can contribute to this classification.\n",
    "\n",
    "In our research, we attempt to apply machine learning techniques to credit analysis, specifically in predicting a person's creditability and in doing so, evaluating which variables are most relevant in defining good and bad payers. The main question at hand here is how do we measure the risk in granting credit to individuals with greater accuracy <a name=\"pinco\"></a>[<sup>[4]</sup>](#pinconote)? Some of the secondary points to consider that will aid us in answering include determining the variables that define the individual's ability to pay, what makes an individual take credit even though they are aware they don't have the resources to pay, and how the individual's behavior as a consumer impact their own creditability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problem involves creating a machine learning model to try and predict a person's creditability by utilizing different variables. In order to create such a model to solve this problem, we need to take into account a range of relevant parameters such as income, age, occupation, and more. This problem is solvable by quantifying critical variables into playing a role in an individual's credit standing. For example, credit standing will be scored in terms of “Bad” or “Good,” which will be dependent on the critical values that factor into creditability. The problem is measurable because creditability has had a long-running background in its form of operation. After comparing the predicted credit standing to the true credit standing, we can then evaluate the accuracy and efficiency of the machine learning model that we plan to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "This credit dataset is from 1994 and was found in the UCI Machine Learning Repository <a name=\"dataset\"></a>[<sup>[5]</sup>](#datasetnote), it contains 1000 observations with 21 different variables. The first attribute called creditability is the variable we will be classifying with the following 20 attributes being the variables that we train our models on. Note that this dataset consists of data scored under the German credit system, which is different from FICO credit scoring as described in the previous section; however, both procedures share many financial statistics used to evaluate a person's credit worthiness.\n",
    "\n",
    "An observation consists of:\n",
    " - Creditability: It has values of 0 or 1 where 0 indicates a person is not credit-worthy and 1 indicates a person is credit-worthy\n",
    " - Account Balance: Balance of the current account in German currency (Deutsche Mark)\n",
    " - Duration of Credit: Credit duration range from <= 6 months to > 54 months\n",
    " - Payment Status of Previous Credit: Status of previous credit payments\n",
    " - Purpose: Purpose of credit which includes cars, furniture, household appliances, education, business, etc.\n",
    " - Credit Amount: Amount of credit in German currency (Deutsche Mark)\n",
    " - Value Savings/Stocks: Value of savings or stocks in German currency (Deutsche Mark)\n",
    " - Length of current employment: How long a person has been employed ranging from <= 1 year to >= 7 years\n",
    " - Installment: Installment in percent of available income\n",
    " - Sex & Marital Status: Sex combined with marital status\n",
    " - Guarantors: Further debtors or guarantors\n",
    " - Duration in Current address: How long a person has been living in their current household for\n",
    " - Most valuable available asset: Most valuable available asset categories include house, land, life insurance, car, or no asset\n",
    " - Age: Age range from  0 to >= 65 years old\n",
    " - Concurrent Credits: Further running credits at other institutions\n",
    " - Type of apartment: Categories include rented flat, owner-occupied flat, or free apartment\n",
    " - Number of Credits at this Bank: ​​Number of previous credits at this bank (including the running one)\n",
    " - Occupation: Categories include unemployed, unskilled worker, skilled worker, executive\n",
    " - Number of dependents: Number of people entitled to maintenance\n",
    " - Telephone: It has values of 1 or 2 where 1 indicates a person does not have a telephone and 2 indicates a person has a telephone\n",
    " - Foreign Worker: It has values of 1 or 2 where 1 indicates a person is a foreign worker and 2 indicates a person is not a foreign worker\n",
    "\n",
    "All of the variables mentioned above are to be considered as critical variables because of their contribution to the main scoring of creditability and are all categorical variables but encoded into numerics. \n",
    "\n",
    "The table below also shows defines whether each variable is categorical or numerical.\n",
    "\n",
    "| Attribute      | Description | Class |\n",
    "| ----------- | ----------- | --------- |\n",
    "| 1      | Creditability (What we're predicting)      | Categorical |\n",
    "| 2   | Account Balance        | Categorical |\n",
    "| 3 | Credit Length (months) | Numeric |\n",
    "| 4 | Status of Payment | Categorical | \n",
    "| 5 | Purpose of Loan | Categorical | \n",
    "| 6 | Credit Amount | Numeric | \n",
    "| 7 | Savings In Cost | Categorical |\n",
    "| 8 | Current Employment Period | Categorical |\n",
    "| 9 | Installment | Numeric |\n",
    "| 10 | Sex and Marital Status | Categorical |\n",
    "| 11 | Guarantors | Categorical | \n",
    "| 12 | Current Address Duration | Numerical |\n",
    "| 13 | Most Precious Resources | Categorical | \n",
    "| 14 | Age | Numeric | \n",
    "| 15 | Simultaneous Loans | Categorical |\n",
    "| 16 | Type of House | Categorical |\n",
    "| 17 | Amount of Loans From This Bank | Numeric | \n",
    "| 18 | Employment | Categorical |\n",
    "| 19 | Number of Dependents | Categorical | \n",
    "| 20 | Telephone | Categorical | \n",
    "| 21 | Foreign Workers | Categorical | \n",
    "\n",
    "As shown, most variables are categorical, which prompts the need for one-hot encoding but the data has already been represented this way. Moreover, there are no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "Prior literature and experiments have demonstrated that the most successful models for credit score classification are logistic regression, neural networks, decision trees, random forests, and support vector machines due to the complexity of many credit card datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "In classification, we are concerned with building robust models by seeing how it well it performs in correctly classifying unseen data. Depending on the selected model, we will be using include accuracy, precision, recall, and F1-score for our performance metrics, and things like false positive rate for our error metric. We prioritize accuracy and F1-score as previously mentioned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Maybe you include a learning curve to show whether you have enough data to do train/validate/test split or have to go to k-folds or LOOCV or ???\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsection 1: Exploratory Data Analysis\n",
    "\n",
    "The dataset consists of a mixture of categorical and numeric variables. The pre-processing of the dataset with a characterization of the dataset where lower significance items were removed and numerical values were categorized. The most relevant attributes for credit risk evaluation was selected using Forward Stepwise Regression in the WEKA tool<a name=\"weka\"></a>[<sup>[6]</sup>](#weka), which also displayed the comparative gain of every variable. Thus, we were able to easily analyze the effects of each variable and their impact on influencing the model's categorization of the data without having to implement multiple models and algorithms on the entire dataset, saving us time. To see which algorithms might seem promising, we perform exploratory data analysis to explore any possible relationships between variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv('German_credit.csv')\n",
    "\n",
    "# Overview of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Summary statistics\n",
    "print(data.describe())\n",
    "\n",
    "# Distribution of credit_risk variable\n",
    "sns.countplot(x='Creditability', data=data)\n",
    "plt.xlabel('Credit Risk')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Credit Risk')\n",
    "plt.show()\n",
    "\n",
    "# Distribution of credit_amount variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(x='Credit Amount', data=data, bins=20, kde=True)\n",
    "plt.xlabel('Credit Amount')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Credit Amount')\n",
    "plt.show()\n",
    "\n",
    "# Relationship between credit_amount and credit_risk\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Creditability', y='Credit Amount', data=data)\n",
    "plt.xlabel('Credit Risk')\n",
    "plt.ylabel('Credit Amount')\n",
    "plt.title('Relationship between Credit Risk and Credit Amount')\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix = data.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also generate some plots to see if any particular variables show an interesting relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('German_credit.csv')\n",
    "\n",
    "# Extract the 'Creditability' and 'Foreign Worker' columns\n",
    "creditability = df['Creditability']   \n",
    "balance = df['Account Balance']\n",
    "\n",
    "# Create a grouped DataFrame to count the occurrences\n",
    "grouped_data = df.groupby(['Account Balance', 'Creditability']).size().unstack()\n",
    "\n",
    "# Plot the histogram\n",
    "grouped_data.plot(kind='bar', stacked=True)\n",
    "\n",
    "# Set the labels and title\n",
    "plt.xlabel('Balance of current account')\n",
    "plt.ylabel('Number of People')\n",
    "plt.title('Does Bank Account Balance Affect Creditworthiness?')\n",
    "\n",
    "# DM is German currency\n",
    "x_labels = ['No Running Account', 'No Balance or Debit', '0 <= ... < 200 DM', '... >= 200 DM or checking account for 1+ year']\n",
    "plt.gca().set_xticklabels(x_labels, rotation=0)\n",
    "\n",
    "# Customize the color scheme\n",
    "colors = ['#FF7F50', '#1E90FF', '#00FF00', '#FF0000']  # Creditability 0: Blue (not credit-worthy) , Creditability 1: Orange (credit-worthy)\n",
    "plt.gca().set_prop_cycle('color', colors)\n",
    "\n",
    "# Rotate x-value labels\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Customize the legend labels\n",
    "legend_labels = ['Not Creditworthy', 'Creditworthy']\n",
    "plt.legend(legend_labels)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsection 2: Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we notice from the distribution credit risk that our data is slightly unbalanced; there are more observations with good creditability (1) than bad creditability (0). Hence, when we are optimizing the performance of our model, we might look to apply some class imbalance techniques to address this issue, but for now, it seems like logistic regression is a good baseline model to work with due to the last graph showing that higher account balance correlates positively with creditworthiness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv('German_credit.csv')\n",
    "\n",
    "# Separate the features and target variable\n",
    "X = data.drop('Creditability', axis=1)\n",
    "y = data['Creditability']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Plot the ROC curve\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsection 3: Model Selection Using Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model doesn't perform very well, however. The ROC-curve is slightly is far from an ideal shape and the model was unable to converge as shown by the warning. In addition, our problem requires us to explore all avenues of machine learning models in order to see which one performs best on classifying the data and to uncover any false assumptions or information we may have overlooked during EDA. Thus, in this section, we will do model selection via $k$-fold cross validation, as our dataset is relatively small with 1000 observations, so the computational budget is considerably reasonable.  \n",
    "\n",
    "Note that one of our models is an artificial neural network developed in PyTorch. It is a custom deep neural network with five layers, two of which are the input and output layers. The input dimension has a feature size of 21 and maps to the second layer with 64 neurons, followed by a mapping to 32 neurons, and finally 2 neurons for the binary classification. We use ReLU for the activation function and softmax to handle multi-class classification and output a probability distribution over such classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the German credit dataset (assuming it's stored in a DataFrame named 'df')\n",
    "df = pd.read_csv('German_credit.csv')\n",
    "\n",
    "# Separate the features and target variable\n",
    "X = df.drop('Creditability', axis=1)\n",
    "y = df['Creditability']\n",
    "\n",
    "# Set up the models with varying hyperparameters\n",
    "dt_params = [{'max_depth': 5}, {'max_depth': 10}, {'max_depth': None}]\n",
    "svm_params = [{'C': 1, 'kernel': 'linear'}, {'C': 1, 'kernel': 'rbf'}, {'C': 10, 'kernel': 'rbf'}]\n",
    "rf_params = [{'n_estimators': 100, 'max_depth': 5}, {'n_estimators': 100, 'max_depth': 10}, {'n_estimators': 200, 'max_depth': 10}]\n",
    "mlp_params = [{'hidden_layer_sizes': (100,), 'activation': 'logistic'}, {'hidden_layer_sizes': (100,), 'activation': 'relu'}, {'hidden_layer_sizes': (200,), 'activation': 'relu'}]\n",
    "\n",
    "models = [\n",
    "    (DecisionTreeClassifier(), dt_params),\n",
    "    (SVC(), svm_params),\n",
    "    (RandomForestClassifier(), rf_params),\n",
    "    (MLPClassifier(), mlp_params)\n",
    "]\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "k = 10\n",
    "\n",
    "# Initialize lists to store the error metrics\n",
    "accuracy_scores = []\n",
    "error_rates = []\n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "sensitivity_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    fold_accuracy_scores = []\n",
    "    fold_error_rates = []\n",
    "    fold_f1_scores = []\n",
    "    fold_precision_scores = []\n",
    "    fold_recall_scores = []\n",
    "    fold_sensitivity_scores = []\n",
    "\n",
    "    for model, params in models:\n",
    "        best_score = -1\n",
    "        best_pred = None\n",
    "\n",
    "        for param in params:\n",
    "            model.set_params(**param)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_pred = y_pred\n",
    "\n",
    "        fold_accuracy_scores.append(best_score)\n",
    "        fold_error_rates.append(1 - best_score)\n",
    "        fold_f1_scores.append(f1_score(y_test, best_pred))\n",
    "        fold_precision_scores.append(precision_score(y_test, best_pred))\n",
    "        fold_recall_scores.append(recall_score(y_test, best_pred))\n",
    "        fold_sensitivity_scores.append(recall_score(y_test, best_pred))\n",
    "\n",
    "    accuracy_scores.append(fold_accuracy_scores)\n",
    "    error_rates.append(fold_error_rates)\n",
    "    f1_scores.append(fold_f1_scores)\n",
    "    precision_scores.append(fold_precision_scores)\n",
    "    recall_scores.append(fold_recall_scores)\n",
    "    sensitivity_scores.append(fold_sensitivity_scores)\n",
    "\n",
    "# Convert the lists to numpy arrays for easier plotting\n",
    "accuracy_scores = np.array(accuracy_scores)\n",
    "error_rates = np.array(error_rates)\n",
    "f1_scores = np.array(f1_scores)\n",
    "precision_scores = np.array(precision_scores)\n",
    "recall_scores = np.array(recall_scores)\n",
    "sensitivity_scores = np.array(sensitivity_scores)\n",
    "\n",
    "# Plotting\n",
    "models_names = ['Decision Tree', 'SVM', 'Random Forest', 'Multi-layer NN']\n",
    "metrics_names = ['Accuracy', 'Error Rate', 'F1 Score', 'Precision', 'Recall', 'Sensitivity']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(10, 20))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.plot(range(1, k+1), accuracy_scores[:, i], marker='o')\n",
    "    ax.set_xlabel('Fold')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(models_names[i])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print Evaluation Metrics\n",
    "columns = ['Model', 'Accuracy', 'Error Rate', 'F1 Score', 'Precision', 'Recall', 'Sensitivity']\n",
    "table_data = []\n",
    "\n",
    "for i in range(len(models_names)):\n",
    "    model_name = models_names[i]\n",
    "    accuracy = np.mean(accuracy_scores[:, i])\n",
    "    error_rate = np.mean(error_rates[:, i])\n",
    "    f1_score = np.mean(f1_scores[:, i])\n",
    "    precision = np.mean(precision_scores[:, i])\n",
    "    recall = np.mean(recall_scores[:, i])\n",
    "    sensitivity = np.mean(sensitivity_scores[:, i])\n",
    "\n",
    "    table_data.append([model_name, accuracy, error_rate, f1_score, precision, recall, sensitivity])\n",
    "\n",
    "table = pd.DataFrame(table_data, columns=columns)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also do further algorithm comparison using paired $t$-tests to further evaluate how specific models compare against each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsection 4: Optimizing the Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our cross-validation and hyperparameter search shows that random forests perform well, specifically with around 100 estimators. Hence, we will continue to optimize the performance of this model. This time, we will normalize the dataset and apply class imbalance t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imblearn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Load the German credit dataset (assuming it's stored in a DataFrame named 'df')\n",
    "df = pd.read_csv('German_credit.csv')\n",
    "\n",
    "# Separate the features and target variable\n",
    "X = df.drop('Creditability', axis=1)\n",
    "y = df['Creditability']\n",
    "\n",
    "# Normalize the dataset using Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Oversample the minority class\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_train_oversampled, y_train_oversampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Undersample the majority class\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_train_undersampled, y_train_undersampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train the Random Forest model with oversampled data\n",
    "rf_model_oversampled = RandomForestClassifier(n_estimators=100, max_depth=10, class_weight='balanced')\n",
    "rf_model_oversampled.fit(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "# Train the Random Forest model with undersampled data\n",
    "rf_model_undersampled = RandomForestClassifier(n_estimators=100, max_depth=10, class_weight='balanced')\n",
    "rf_model_undersampled.fit(X_train_undersampled, y_train_undersampled)\n",
    "\n",
    "# Make predictions on the test set using oversampled model\n",
    "y_pred_oversampled = rf_model_oversampled.predict(X_test)\n",
    "\n",
    "# Make predictions on the test set using undersampled model\n",
    "y_pred_undersampled = rf_model_undersampled.predict(X_test)\n",
    "\n",
    "# Calculate the evaluation metrics for oversampled model\n",
    "accuracy_oversampled = accuracy_score(y_test, y_pred_oversampled)\n",
    "f1_oversampled = f1_score(y_test, y_pred_oversampled)\n",
    "error_rate_oversampled = 1 - accuracy_oversampled\n",
    "sensitivity_oversampled = recall_score(y_test, y_pred_oversampled)\n",
    "precision_oversampled = precision_score(y_test, y_pred_oversampled)\n",
    "\n",
    "# Calculate the evaluation metrics for undersampled model\n",
    "accuracy_undersampled = accuracy_score(y_test, y_pred_undersampled)\n",
    "f1_undersampled = f1_score(y_test, y_pred_undersampled)\n",
    "error_rate_undersampled = 1 - accuracy_undersampled\n",
    "sensitivity_undersampled = recall_score(y_test, y_pred_undersampled)\n",
    "precision_undersampled = precision_score(y_test, y_pred_undersampled)\n",
    "\n",
    "# Print the evaluation metrics for oversampled model\n",
    "print(\"Evaluation Metrics - Oversampled Model:\")\n",
    "print(\"Accuracy:\", accuracy_oversampled)\n",
    "print(\"F1 Score:\", f1_oversampled)\n",
    "print(\"Error Rate:\", error_rate_oversampled)\n",
    "print(\"Sensitivity:\", sensitivity_oversampled)\n",
    "print(\"Recall:\", sensitivity_oversampled)\n",
    "print(\"Precision:\", precision_oversampled)\n",
    "\n",
    "# Print the evaluation metrics for undersampled model\n",
    "print(\"Evaluation Metrics - Undersampled Model:\")\n",
    "print(\"Accuracy:\", accuracy_undersampled)\n",
    "print(\"F1 Score:\", f1_undersampled)\n",
    "print(\"Error Rate:\", error_rate_undersampled)\n",
    "print(\"Sensitivity:\", sensitivity_undersampled)\n",
    "print(\"Recall:\", sensitivity_undersampled)\n",
    "print(\"Precision:\", precision_undersampled)\n",
    "\n",
    "# Confusion matrix for oversampled model\n",
    "confusion_mat_oversampled = confusion_matrix(y_test, y_pred_oversampled)\n",
    "print(\"Confusion Matrix - Oversampled Model:\")\n",
    "print(confusion_mat_oversampled)\n",
    "\n",
    "# Confusion matrix for undersampled model\n",
    "confusion_mat_undersampled = confusion_matrix(y_test, y_pred_undersampled)\n",
    "print(\"Confusion Matrix - Undersampled Model:\")\n",
    "print(confusion_mat_undersampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsection 5: A New Approach, Neural Network Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an extension to this model to see if we can further improve upon our "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imblearn\n",
    "\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv('German_credit.csv')\n",
    "\n",
    "# Separate the features and target variable\n",
    "X = data.drop('Creditability', axis=1)\n",
    "y = data['Creditability']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Undersample the majority class\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_train_undersampled, y_train_undersampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Oversample the minority class using SMOTE\n",
    "oversampler = SMOTE(random_state=42)\n",
    "X_train_oversampled, y_train_oversampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Class weighting\n",
    "class_weights = {\n",
    "    0: len(y_train) / (2 * sum(y_train == 0)),\n",
    "    1: len(y_train) / (2 * sum(y_train == 1))\n",
    "}\n",
    "\n",
    "# Train and evaluate a random forest classifier with undersampled data\n",
    "clf_undersampled = RandomForestClassifier(random_state=42)\n",
    "clf_undersampled.fit(X_train_undersampled, y_train_undersampled)\n",
    "y_pred_undersampled = clf_undersampled.predict(X_test)\n",
    "print(\"Classification Report - Undersampled Data:\")\n",
    "print(classification_report(y_test, y_pred_undersampled))\n",
    "\n",
    "# Train and evaluate a random forest classifier with oversampled data\n",
    "clf_oversampled = RandomForestClassifier(random_state=42)\n",
    "clf_oversampled.fit(X_train_oversampled, y_train_oversampled)\n",
    "y_pred_oversampled = clf_oversampled.predict(X_test)\n",
    "print(\"Classification Report - Oversampled Data:\")\n",
    "print(classification_report(y_test, y_pred_oversampled))\n",
    "\n",
    "# Train and evaluate a random forest classifier with class weights\n",
    "clf_weighted = RandomForestClassifier(random_state=42, class_weight=class_weights)\n",
    "clf_weighted.fit(X_train, y_train)\n",
    "y_pred_weighted = clf_weighted.predict(X_test)\n",
    "print(\"Classification Report - Weighted Data:\")\n",
    "print(classification_report(y_test, y_pred_weighted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "Since our project focuses on credit classification, several ethical and privacy concerns must be considered. First, we must consider data privacy and security since personal financial data associated with classifying credit score is confidential and highly sensitive. If we were given personally identifiable information (which can be insecure if given in the correct combinations, i.e. birthday, SSN, and address), we would have to scrap these variables or encrypt them so they are not visible during the EDA process. However, we will not be using these variables for our application, as these variables are usually provided to a third-party financial institution to perform background checks to ensure accurate credit scoring. However, the data we collected will not contain this information; instead, it contains non-personally sensitive information such as occupation and amount in the bank account. When given in combination, it is rare that such data can be used to expose customer data. Regardless, we will be asking for consent if we were to collect this data in real life. Other than that, the data set that we will be using is completely safe and customer personal information will be invisible to us and other potential adversaries.\n",
    "\n",
    "Another concern is the biases that may emerge from an imbalance in our data collection or biased decision-making during the data labeling process. We plan to address this concern by building models that are fair and unbiased, while actively working to identify and mitigate any unfair impacts. For instance, there are signficantly more foreign workers than non-foreign workers in out data set, so perhaps we may not consider this variable or build a fair training and test split from it. Lastly, we might face concerns regarding transparency so we plan to make sure sure that the models used for classification to be transparent and explainable. Some machine learning algorithms that we use in our work, such as deep learning models, are often considered black-box models, making it challenging to interpret their decision-making process. Credit risk assessment requires transparency and explainability, as financial institutions need to understand the reasons behind credit risk predictions to comply with regulations and provide justifications for their decisions.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Modern global markets are full of risks and "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"abdounote\"></a>1.[^](#abdou): Abdou, HAH and Pointon, J. (2011). Credit scoring, statistical techniques and evaluation criteria: A review of the literature, Intelligent Systems in Accounting, Finance Management. 18 (2-3), pp. 59-88.<br> \n",
    "<a name=\"xiaonote\"></a>2.[^](#xiao): XIAO.-L. Li and Y. Zhong. (2012). An overview of personal credit scoring: Techniques and future work, ‖ International Journal of Intelligence Science, vol. 2, no. 4, pp. 181–189.<br>\n",
    "<a name=\"ficonote\"></a>3.[^](#fico): https://www.fico.com/en/products/fico-score.<br>\n",
    "<a name=\"pinconote\"></a>4.[^](#pinco): M. Pincovsky, A. Falcão, W. N. Nunes, A. Paula Furtado and R. C. L. V. Cunha, \"\"Machine Learning applied to credit analysis: a Systematic Literature Review\",\" 2021 16th Iberian Conference on Information Systems and Technologies (CISTI), Chaves, Portugal, 2021, pp. 1-5, doi: 10.23919/CISTI52073.2021.9476350.<br>\n",
    "<a name=\"dataset\"></a>5.[^](#dataset): https://online.stat.psu.edu/stat857/node/222/<br> \n",
    "<a name=\"weka\"></a>6.[^](#weka): https://www.cs.waikato.ac.nz/ml/weka/<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit\n",
    "\n",
    "We strongly believe we deserve extra credit on this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
